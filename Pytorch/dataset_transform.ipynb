{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "dataset_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root=\"./dataset\", train=True, transform=dataset_transform, download=True)\n",
    "test_set = torchvision.datasets.CIFAR10(root=\"./dataset\", train=False, transform=dataset_transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.6039, 0.4941, 0.4118,  ..., 0.3569, 0.3412, 0.3098],\n",
       "          [0.5490, 0.5686, 0.4902,  ..., 0.3765, 0.3020, 0.2784],\n",
       "          [0.5490, 0.5451, 0.4510,  ..., 0.3098, 0.2667, 0.2627],\n",
       "          ...,\n",
       "          [0.6863, 0.6118, 0.6039,  ..., 0.1647, 0.2392, 0.3647],\n",
       "          [0.6471, 0.6118, 0.6235,  ..., 0.4039, 0.4824, 0.5137],\n",
       "          [0.6392, 0.6196, 0.6392,  ..., 0.5608, 0.5608, 0.5608]],\n",
       " \n",
       "         [[0.6941, 0.5373, 0.4078,  ..., 0.3725, 0.3529, 0.3176],\n",
       "          [0.6275, 0.6000, 0.4902,  ..., 0.3882, 0.3137, 0.2863],\n",
       "          [0.6078, 0.5725, 0.4510,  ..., 0.3216, 0.2745, 0.2706],\n",
       "          ...,\n",
       "          [0.6549, 0.6039, 0.6275,  ..., 0.1333, 0.2078, 0.3255],\n",
       "          [0.6039, 0.5961, 0.6314,  ..., 0.3647, 0.4471, 0.4745],\n",
       "          [0.5804, 0.5804, 0.6118,  ..., 0.5216, 0.5255, 0.5216]],\n",
       " \n",
       "         [[0.7333, 0.5333, 0.3725,  ..., 0.2784, 0.2784, 0.2745],\n",
       "          [0.6627, 0.6039, 0.4627,  ..., 0.3059, 0.2431, 0.2392],\n",
       "          [0.6431, 0.5843, 0.4392,  ..., 0.2510, 0.2157, 0.2157],\n",
       "          ...,\n",
       "          [0.6510, 0.6275, 0.6667,  ..., 0.1412, 0.2235, 0.3569],\n",
       "          [0.5020, 0.5098, 0.5569,  ..., 0.3765, 0.4706, 0.5137],\n",
       "          [0.4706, 0.4784, 0.5216,  ..., 0.5451, 0.5569, 0.5647]]]),\n",
       " 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_set[0]\n",
    "# img.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preporcessing example for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"logs\")\n",
    "for i in range(10):\n",
    "    img, traget = train_set[i]\n",
    "    writer.add_image(\"train_set\", img, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Computer_Vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
